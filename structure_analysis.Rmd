---
title: "Geographic Prediction of P.Falciparum isolates in PNG"
author: "Stuart Lee"
date: "7 March 2016"
output:
  html_document:
    fig_caption: yes
    toc: yes
    keep_md: true
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
               fig.path = "./figures/", 
               cache.path = "./cache/",
               fig.align = 'center')
library(dplyr)
library(tidyr)
library(ggplot2)
library(caret)
devtools::install_github("sa-lee/starmie")
library(starmie)
theme_set(theme_bw())
```

# Introduction

We have implemented several prediction models that attempt to find a minimal set of 
_P.falciparum_ SNPs that are predictive of geography in Papua New Guinea. The goal of these
models is to aid in fine-tuning a SNP bar code assay that is being developed to genotype
_P.falciparum_ infections in PNG. Ultimately, the model will be used both in 
the design of the final barcode and in the field for prediction of new infections.


## Data Background and Processing
```{r read_in, cache=TRUE}
pf_snps <- read.csv("MAL192_154_SNPs.csv", stringsAsFactors = FALSE)
# recode -9 as missing
pf_snps[pf_snps == -9] <- NA
# drop samples with genotype conflicts in replicate SNPs
# recode factor variable as character
pf_snps_recode <- pf_snps %>% 
  filter(!grepl("\\*$", Sample)) %>% 
  mutate(Provincial_layer = gsub(" ", "", as.character(Provincial_layer)))

```
The data was obtained by sampling `r nrow(pf_snps)` _P.falciparum_ infections (including duplicates) in 
`r unique(pf_snps_recode$Provincial_layer)  %>% length` distinct locations ('Provincial layers') within
PNG. All infections were genotyped using the Fluidigm platform consisting of 191 SNPs (with 1 SNP replicated between plates for QC purposes) obtained from previous work using WGS data. 37 SNPs failed genotyping
completely (either due of high missingness rates or inconsistencies), leaving 154 SNPs left. We have also decided to exclude 14 taxa where there was replication failure on the control SNP between plates.
Figure 1 shows the number of SNPs with within each taxa that have either the reference or alternate allele or a missing genotype call.


```{r missing_data, fig.cap="Bar plot of SNP missingess in each taxa."}
# work out proportion missingess in each SNP
by_sample_missing <- pf_snps_recode %>% 
  dplyr::select(Sample, starts_with("Pf")) %>% 
  gather(snp, genotype, -Sample)  %>% 
  count(Sample, genotype)

# create stacked bar plot
missingness_bar <- ggplot(by_sample_missing, aes(x = Sample, y = n, fill = factor(genotype))) +
  geom_bar(stat = "identity") + scale_y_continuous(expand = c(0,0)) + 
  scale_fill_discrete("Genotype") +
  theme(axis.text.x = element_blank(), panel.grid = element_blank())

missingness_bar
```

Table 1 shows the relationships between the Provincial Layer,
Geographical Groupings (i.e. village level). We have also shown below how these
categories correspond to the latent population structure inferred
using STRUCTURE. 

![Geography of PNG and surrounding regions.](PNGmap.png)

```{r geography-layout}
# isolates by geography, 3 layers 
# 1 - predicted by structure
# 2 - Geographic (village level)
# 3 - Provincial_layer
pf_snps_recode %>% 
  count(Provincial_layer, Geographic) %>% 
  arrange(desc(n)) %>% 
  knitr::kable(format = "markdown", align = "c")

pf_recode2 <- pf_snps_recode %>% 
  filter(!(Provincial_layer %in% c("UN", "SthCoast")), 
         !(Geographic %in% c("8.2", "8.5", "30"))) %>% 
  dplyr::select(-Subpop)

# recode vars from Alyssa's advice
pf_recode2$Provincial_layer[pf_recode2$Geographic == '17'] <- '17'
pf_recode2$Provincial_layer[pf_recode2$Geographic == '24'] <- '23'
    
pf_recode2 <- pf_recode2 %>%  
    mutate(Provincial_layer = factor(Provincial_layer)) 
```

There are several samples where the origin of the infection is unknown or where 
there are low counts in some regions. Upon advice from Abby I have excluded taxa 
from unknown locations (EXXON samples), the South Coast, or from highland regions 8.2 and 8.5.

Furthermore, advice from Alyssa, suggested the removal of isolates from the 
Solomon Islands (region = 30) as that population is highly inbred. This
leaves `r nrow(pf_recode2)` taxa. There also appears to be some potential label 
swaps, where the Geographic Label (GL) has been placed into the incorrect 
Provincial layer (PL). This includes taxa where from GL 24 (Bougainville) that 
are placed in PL 26, when they  correspond to  taxa from the outer islands and 
should be put in PL 23. Furthermore, the two infections from GL 17 (Central
Province) should be separated into its own PL, which we can merge into a 
broader category later.

## Population Structure Inference
We decided to rerun STRUCTURE after excluding the problematic samples and 
markers using the following settings:

1. Run STRUCTURE with K= 1 up to 20 with 20 runs with different seeds.
2. Use an MCMC burn-in period of 5000 iterations and total MCMC iterations up to 50,000.
3. Use inflection point in final estimated log-likelihood to determine optimal K
   and look at results of 'evanno' method.
4. Check MCMC diagnostics by looking at change in admixture parameter over MCMC runs.

All the following analysis and visualizations were performed in our new R package `starmie` (https://github.com/sa-lee/starmie).

```{r eval-structure}
# check if structure has been run 
not_run <- !file.exists("structure_runs_MAL192.rds")

# if we haven't stored results of structure run, run the pipeline.
# and extract data for structure.
if( not_run ) {
    # recode  NA to -9
    str_df <- pf_recode2
    str_df[is.na(str_df)] <- -9
    str_df <- str_df %>% dplyr::select(-Geographic)
    write.table(str_df, "MAL192.str", 
                quote = FALSE, row.names = FALSE, col.names = FALSE) 
} else {
    str_filelist <- readRDS("structure_runs_MAL192.rds")
}

```

```{r run_structure, child="run_structure.Rmd", eval=not_run}
```

```{r starmie-mcmc, fig.width=18, fig.height = 14}

# check MCMC diagnostics
mcmc_df <- do.call("rbind", lapply(1:400, 
                                   function(i) getMCMC(str_filelist[[i]])))

mcmc_df <- mcmc_df %>% group_by(K, Iteration) %>% 
    mutate(run = row_number()) %>% 
    filter(run <= 20)

ggplot(mcmc_df, 
       aes(x = Iteration, y = Alpha, colour = factor(run))) + 
    facet_grid(K ~., scales = "free_y") + geom_line()

```



From the MCMC diagnostic plots it appears that the admixture coefficient $\alpha$
is relatively stable over the chains for all $K$ values except in the case 
of $K = 1$ (where it doesn't really make sense.).

```{r starmie-mcmc2, fig.width=18, fig.height=14}
ggplot(mcmc_df, 
       aes(x = Iteration, y= LogL, colour = factor(run))) + 
    facet_grid(K ~., scales = "free_y") + geom_line() 

```

Similarly, the likelihood estimates appear to be relatively stable over the runs for 
each $K$, and the chains appear to have converged at the final iteration, 
although a longer burn-in  period is required for the $K > 10$ runs.


```{r logL}
best_k_strucutre <- bestK(str_filelist, "structure")

```

We estimate `best K` by simply plotting the log posterior probability of
the data for each $K$ and look for the inflection point in the curve. STRUCTURE
computes the estimated log posterior probability by taking the ratio of the mean 
estimated log-likelihood of the data and the estimated variance of the 
log-likelihood of  the data over all MCMC chains. We use the Evanno method to 
look at the change in log-likelihood for each $K$ over all runs. 


```{r evanno, fig.width = 12, fig.height=8}
# find 'best k'
best_k_evanno <- bestK(str_filelist, "evanno")
```

After inspection of the curves, $K$ = 8 looks like a reasonable estimate of
the number of sub-populations in the data. We then CLUMPP the results together
and plot the average ancestral membership coefficients over the runs by partitioning 
by the original  population label in the data set. 

```{r clummp, fig.width=16, fig.height=12}
k8_runs <- str_filelist[c(141:160)]
stopifnot(all(unlist(lapply(k8_runs, getK)) == 8))

Q_matrices <- lapply(k8_runs, getQ)

# bug in CLUMPP, not run
Qclumpp <- clumpp(Q_matrices, "largeKgreedy")

Q_df <- Q_matrices[[1]]
Q_df <- data.frame(Sample = rownames(Q_df), Q_df, stringsAsFactors = FALSE)
Q_df <- gather(Q_df, key = Cluster, value = Proportion, -Sample)
# join to samples
Q_final <- left_join(Q_df, pf_recode2 %>% 
                         dplyr::select(Sample, Geographic, Provincial_layer), 
                     by = "Sample")


ggplot(Q_final, aes(x = Sample, y = Proportion, fill = factor(Cluster))) + 
facet_grid(Cluster ~ Provincial_layer, scales = "free_x", space = "free_x") +
geom_bar(stat = "identity", width=1) + 
scale_y_continuous(expand=c(0,0), breaks=c(0.25,0.75)) +
theme(axis.text.x = element_blank(),
      legend.position = "bottom",
      legend.title = element_blank())

```

We also zoom on a few PLs where there appear to be outliers to see the 
correspondence between GL and STRUCTURE populations. 

```{r struct-contignecy, fig.cap="Samples in PLs 13, 7, and 4 appear to have several outlier samples, indicating these should be separated into a new cluster."}

# hard_cluster_counts <- Q_final %>% 
#     group_by(Sample) %>% 
#     filter(Proportion == max(Proportion)) %>% 
#     count(Provincial_layer, Cluster)
# 
# kable(hard_cluster_counts)
# PLs where weird stuff is going on
ggplot(Q_final %>% filter(Provincial_layer == '13'),
       aes(x = Sample, y = Proportion, fill = factor(Cluster))) +
facet_grid(Cluster ~ Geographic, scales = "free_x", space = "free_x") +
geom_bar(stat = "identity", width=1) + 
scale_y_continuous(expand=c(0,0), breaks=c(0.25,0.75)) +
theme(axis.text.x = element_blank(),
      legend.position = "bottom",
      legend.title = element_blank())

ggplot(Q_final %>% filter(Provincial_layer == '4'),
       aes(x = Sample, y = Proportion, fill = factor(Cluster))) +
facet_grid(Cluster ~ Geographic, scales = "free_x", space = "free_x") +
geom_bar(stat = "identity", width=1) + 
scale_y_continuous(expand=c(0,0), breaks=c(0.25,0.75)) +
theme(axis.text.x = element_blank(),
      legend.position = "bottom",
      legend.title = element_blank())  

ggplot(Q_final %>% filter(Provincial_layer == '7'),
       aes(x = Sample, y = Proportion, fill = factor(Cluster))) +
facet_grid(Cluster ~ Geographic, scales = "free_x", space = "free_x") +
geom_bar(stat = "identity", width=1) + 
scale_y_continuous(expand=c(0,0), breaks=c(0.25,0.75)) +
theme(axis.text.x = element_blank(),
      legend.position = "bottom",
      legend.title = element_blank())  


# save weirdo samples
weird_samples <- c((Q_final  %>% 
                        filter(Cluster == "Cluster.8", Proportion > 0.5))$Sample,
                   (Q_final  %>% 
                        filter(Cluster == "Cluster.4", Proportion > 0.5))$Sample)

```


We also see from the STRUCTURE bar plots that Provincial layers 4 and 7 are mostly mixtures
of clusters 5 and 6, which could correspond to the three sampling regions
in the PNG highlands, Madang and along the border around the Southern highlands.
Furthermore, Provincial layer 1 which is also in the highlands is also mostly a mixture
of clusters 5 and 6.  Provincial layers 16 corresponds to PNG lowlands 
and is mostly composed of cluster 2. Provincial layer 13 which corresponds to
Morobe
is a mixture of clusters 1,2,5 and 6. There also appear to be outlier taxa
derived from cluster 8. Provincial layers 20, 21, 22,  and 23, 
which correspond to East Britian and
New Ireland (and surrounding island regions) are mostly mixtures of clusters
1,2 and 3. Provincial layer 26 which is composed of the lower coast surrounding
Milne Bay is comprised mostly of cluster 7. Provincial layer 19 appears to be
distinct from the other locations where it is mostly comprised of cluster 1.

Since the outliers in PL13 are found in cluster 8 we separate them into their
own category for the purpose of prediction as they are most likely infections
from elsewhere. 
```{r recode-by-structure}

pf_recode2$Region <- NA

pf_recode2$Region[pf_recode2$Provincial_layer %in% c("1", "4", "7")] <- "north_coast"
pf_recode2$Region[pf_recode2$Provincial_layer %in% c("20", "21", "22", "23")] <- "outlying_islands"
pf_recode2$Region[pf_recode2$Provincial_layer == '19']  <- "manus"
pf_recode2$Region[pf_recode2$Provincial_layer %in% c('16', '17')]  <- "eastern_central"

pf_recode2$Region[pf_recode2$Provincial_layer == "13"] <- "morobe"
pf_recode2$Region[pf_recode2$Provincial_layer == "26"] <- "milne_bay"
pf_recode2$Region[pf_recode2$Sample %in% (Q_final  %>% filter(Cluster == "Cluster.8", Proportion > 0.3))$Sample] <- "morobe_imported"


pf_recode2 <- pf_recode2 %>% dplyr::select(Sample, Provincial_layer, 
                                    Geographic, Region, everything())
```


The STRUCTURE output provides guidance for reducing the Provincial layer category
into 6 categories -

1. Provincial layers 1,4,7 forming the "north-coast"
2. Provincial layer 16, 17 forming "eastern-central"
3. Provincial layer 13 forming "morobe"
4. Provincial layer 20-23 forming "outlying-islands"
5. Provincial layer 26 remaining as "milne-bay"
6. Provinicial layer 19 forming "manus"
7. Outlier samples from provinicial layer 13 forming "morobe-imported"


```{r print-table}

pf_recode2 %>% count(Region) %>% knitr::kable(format = "markdown")

```

    
## Data Splitting and EDA

We use the `caret` package to develop our predictive models and investigate SNP
importance on geography.  To begin we have used the 'Provincial layer' as the 
dependent variable, excluding the other geographical labels. We have then split
the `nrow(pf_snps_recode)` taxa into a training (80%) and 
test set (20%). We have tried modelling using support vector machines (SVM) 
with a linear kernel. The out-of sample estimated accuracy using this approach
can be used to form a baseline for testing other models.

Taxa that are from provincial layers '1' and '19' have missing genotype calls 
across all SNPs. 
Hence in order to train models to include these labels (without dropping them 
completely), we need to impute genotypes. 
At the moment we have simply used a median imputation, although there are
probably more appropriate options here. 

```{r recode}
# drop structure and geography codes
# use simple median imputation accross columns
# genetic covariance

median_impute <- preProcess(pf_recode2[, -c(1:4)], method = "medianImpute")
         
pf_recode3 <- predict(median_impute, pf_recode2)

```

To see if the SNPs are informative for population labeling we perform principal
components analysis on the genotype calls. From the PCA plots we see that a group
of  samples from (Provincial_layer = 13) form a distinct cluster away from
other members in the group. Moreover, samples from 
the highlands (Provincial_layer = 4 or 7) in PNG also
form a distinct cluster. The remaining samples seem to clump together with
the direction of the PC1 corresponding to the move from the coast through lowlands
up to the highlands. There also appear to be a few outlier samples in 
Provincial_layer 13 and 4 which correspond to the outliers in 
the structure bar plots above.

```{r pca-complete}

pca_process <- preProcess(pf_recode3[, -c(1:4)], method = "pca")
pca_process
pca_fit <- predict(pca_process, pf_recode3)

# bug in outlier sample labelling here let's fix that

outlier_df <- inner_join(pca_fit %>% 
                            select(Sample, Provincial_layer, Region, PC1, PC2, PC3),
                        data.frame(Sample = weird_samples, Label = weird_samples,
                                   stringsAsFactors = FALSE))

ggplot(pca_fit, 
       aes(x = PC1, y = PC2, colour = Provincial_layer)) + 
    geom_point() +
    geom_text(aes(x = PC1, y = PC2, label = Label), 
              data = outlier_df, vjust = 0, nudge_y = 0.5)
ggplot(pca_fit, 
       aes(x = PC1, y = PC2, colour = Region)) + 
    geom_point() +
    geom_text(aes(x = PC1, y = PC2, label = Label), 
              data = outlier_df, vjust = 0, nudge_y = 0.5)

ggplot(pca_fit, aes(x = PC2, y = PC3, colour = Provincial_layer)) +
    geom_point() +
        geom_text(aes(x = PC2, y = PC3, label = Label), 
              data = outlier_df, vjust = 0, nudge_y = 0.5)
ggplot(pca_fit, aes(x = PC2, y = PC3, colour = Region)) + geom_point() +
        geom_text(aes(x = PC2, y = PC3, label = Label), 
              data = outlier_df, vjust = 0, nudge_y = 0.5)
ggplot(pca_fit, aes(x = PC3, y = PC1, colour = Provincial_layer)) + geom_point() +
        geom_text(aes(x = PC3, y = PC1, label = Label), 
              data = outlier_df, vjust = 0, nudge_y = 0.5)
ggplot(pca_fit, aes(x = PC3, y = PC1, colour = Region)) + geom_point() +
        geom_text(aes(x = PC3, y = PC1, label = Label), 
              data = outlier_df, vjust = 0, nudge_y = 0.5)

```

# Approach 1 -  Support Vector Machine


```{r split, cache=TRUE}
set.seed(20160308)
# split data set
train_indexes <- createDataPartition(pf_recode3$Provincial_layer,
                                     p = 0.8,
                                     list = FALSE)

# drop geographic here
train_df <- pf_recode3[train_indexes, ] %>% 
    dplyr::select(-Geographic, -Sample, -Region) 
test_df <- pf_recode3[-train_indexes, ] %>% 
    dplyr::select(-Geographic, -Sample, -Region)
``` 

First, we fit a support vector machine with a simple linear kernel across all
labels.  For each model we fit, we use 5-fold cross validation repeated 5 times 
to estimate hyper-parameters and within-sample error.
For multiclass SVM, the algorithm creates dummy binary variables for each category
and fits an SVM for each dummy variable. Then to classify a new taxa, we take the
most likely classification over all SVMs.

```{r svm, cache=TRUE}
fitting_controls <- trainControl(method = "repeatedcv", 
                                 number = 5, 
                                 repeats = 5)

svm_fit1 <- train(Provincial_layer ~ ., data = train_df, 
                 method = "svmLinear2",
                 metric = "Kappa",
                 trControl = fitting_controls)

svm_fit1
```

The estimated accuracy of classification on the training set is approximately 
`r round(max(svm_fit1$results$Accuracy), 2)`\%. 
To evaluate the model we estimate the confusion matrix on the test data set using the 
fitted SVM to see how well our model generalises to the test set.

```{r svm_eval}
cm1 <- confusionMatrix(predict(svm_fit1, test_df), test_df$Provincial_layer)

knitr::kable(cm1$table, format = "markdown", align = "c")

```

The accuracy is on the test data set is similar to the training data set.
at `r round(cm1$overall[1], 2) * 100`\%,
We also that the misclassfications from our confusion matrix correspond well 
to the inferred Regions we found from our STRUCTURE analysis.

Using our inferred regions from STRUCTURE, we rebuild our SVM classifier using
the `Region` label constructed above as the dependent variable.
We rebuild a linear SVM with this new labeling.

```{r svm_region, cache = TRUE}
set.seed(999351)
# split data set
train_indexes <- createDataPartition(pf_recode3$Region,
                                     p = 0.8,
                                     list = FALSE)

# drop geographic here
train_df2 <- pf_recode3[train_indexes, ] %>% 
    dplyr::select(-Geographic, -Sample, -Provincial_layer) 
test_df2 <- pf_recode3[-train_indexes, ] %>% 
    dplyr::select(-Geographic, -Sample, -Provincial_layer) 

svmGrid <- expand.grid(cost = c(0.1, 0.25, .5, 0.75, 1), gamma = c(2,3,4))

fitting_controls <- trainControl(method = "repeatedcv",
                                 number = 5,
                                 repeats = 5,
                                 classProbs = TRUE,
                                 summaryFunction = multiClassSummary)

svm_fit2 <- train(Region ~ ., data = train_df2, 
                 method = "svmLinear2",
                 trControl = fitting_controls,
                 metric = "Kappa",
                 tuneGrid = svmGrid)

# keep same resampling metrics accross classes
fitting_controls <- svm_fit2$control

cm2 <- confusionMatrix(predict(svm_fit2, test_df2), test_df2$Region)
knitr::kable(cm2$table, format = "markdown", align = "c")
```

By reducing the number of labels, we have increased the accuracy on our test
set to about `r round(max(svm_fit2$results$Accuracy), 2) *100`\%. We also see
an increase in the estimated test set accuracy `r round(cm2$overall[1], 2)*100`.
From the confusion matrix we see that the SVM has trouble classifying taxa
from Morobe, North Coast and Outlying islands which are the samples that
clump together in the PCA plots.

To account for potential non-linear effects
of SNPs we try fitting an SVM with a Gaussian kernel function. This has an advantage
of increasing the dimensionality of our feature set and potentially better separating
the clumped together samples in Morobe, North Coast and Outlying islands.

```{r svm_fit3}

svm_fit3 <- train(Region ~ ., data = train_df2, method = "svmRadial", 
                  trControl = fitting_controls,
                  metric = "Kappa")

#svm_fit3
cm3 <- confusionMatrix(predict(svm_fit3, test_df2), test_df2$Region)
knitr::kable(cm3$table)
```

Using a Gaussian kernel we obtain a similar training set accuracy of 
`r max(svm_fit3$results$Accuracy) * 100`\%. Furthermore, the estimated test-set
accuracy has increased slightly to `r round(cm3$overall[1], 2)*100`\%. 

The confusion matrix appears to show that using the Gaussian kernel we have
a more sensitive but less specific model to differentiate samples from the 
North Coast, Outlying islands and Morobe, when compared to a linear SVM.

```{r }
byClass_cm3 <- data.frame(cm3$byClass) %>% add_rownames("class") %>% 
    mutate(model = "svmGaussian", snps = "all", n.snps = 154)
byClass_cm2 <- data.frame(cm2$byClass) %>% add_rownames("class") %>% 
    mutate(model = "svmLinear", snps =  "all", n.snps = 154)

combined_cm <- bind_rows(byClass_cm2, byClass_cm3) %>% 
    dplyr::select(-snps) %>% 
    arrange(class, model) %>% 
    dplyr::select(model, class, everything())
knitr::kable(combined_cm, digits = 2, format = "markdown", align = "c")
```

### SNP subset analysis

The SNPs chosen for the Fluidigm assay fall into three broad categories
```{r}
cols <- colnames(test_df2)[2:ncol(test_df2)]
iSNPs <- grepl("i", cols) 
rSNPs <- grepl("r", cols)
vSNPs <- grepl("V", cols)
i_df <- train_df2 %>% dplyr::select(Region, contains("i"))
r_df <- train_df2 %>% dplyr::select(Region, contains("r"))
v_df <- train_df2 %>% dplyr::select(Region, contains("V"))
itest_df <- test_df2 %>% dplyr::select(Region, contains("i"))
rtest_df <- test_df2 %>% dplyr::select(Region, contains("r"))
vtest_df <- test_df2 %>% dplyr::select(Region, contains("V"))

```

```{r pca_subsets, include = FALSE}
pca_wgs <- read_csv("pca_wgs.txt") %>% 
    mutate(Province = ifelse(Region == "Alotau", "Milne Bay",
                             ifelse(Region == "Maprik", "East Sepik", "Madang")))

pca_wgs_all <- preProcess(pca_wgs %>% select(-Region, -sample_id, -Province),
                          method = "pca", pcaComp = 5)

pca_wgs_pred <- predict(pca_wgs_all, pca_wgs) 
pca_wgs_pred$snp_set <- "WGS"
# ggplot(pca_wgs_pred, aes(x = PC1, y = PC2, colour = Province)) + geom_point()
# outlier_samples <- (pca_wgs_pred  %>% filter(PC1 < -10|PC2>20))$sample_id
# 
# pca_wgs_sub <- pca_wgs %>% filter(!(sample_id %in% outlier_samples))
# 
# snps_only <- colnames(pca_wgs_sub)[-c(ncol(pca_wgs_sub), ncol(pca_wgs_sub)-1)]
# freqs <- lapply(snps_only, function(x) (pca_wgs_sub[, x] %>% table() %>% prop.table())[1]) %>% unlist()
# 
# snps_to_keep <- snps_only[which(freqs < 1)]
# 
# pca_wgs_sub2 <- pca_wgs_sub[, c(snps_to_keep, "Region", "sample_id")]
# pca_wgs_sub_fit <- preProcess(pca_wgs_sub2 %>% select(-Region, -sample_id),
#                           method = "pca", pcaComp = 5)
# pca_wgs_sub_pred <- predict(pca_wgs_sub_fit, pca_wgs_sub2)
# ggplot(pca_wgs_sub_pred, aes(x = PC1, y = PC2, colour = Region)) + geom_point()

#pca_wgs$Region2 <- ifelse(pca_wgs$Region == "Maprik", "north_coast",
#                          ifelse(pca_wgs$Region == "Alotau", 
# "milne_bay", #"eastern_central"))

#pca_wgs <- pca_wgs %>% select(-Region) %>% select(Region = Region2, everything())

# run PCA on subset of Fluidig %>% m samples using matching WGS samples
pf_subset <- pf_recode3[!grepl("^BN", pf_recode3$Sample) , ] %>% dplyr::select(Region, Provincial_layer, starts_with("Pf3D7"))
i_all <- pf_subset %>% dplyr::select(Region, Provincial_layer, contains("i"))
r_all <- pf_subset %>% dplyr::select(Region, Provincial_layer, contains("r"))
v_all <- pf_subset %>% dplyr::select(Region, Provincial_layer, contains("V"))

pca_process <- preProcess(pf_subset[, -c(1,2)], 
                          method="pca", pcaComp = 5)
pca_process
pca_fit_all <- predict(pca_process, pf_subset)
pca_fit_all$snp_set = "PNG Barcode (all)"


pca_iall <- preProcess(i_all[,-c(1,2)], method = "pca", pcaComp = 5)
pca_ifit <- predict(pca_iall, i_all)
pca_ifit$snp_set = "PNG Barcode (iSNPs)"

pca_rall <- preProcess(r_all[,-c(1,2)], method = "pca",  pcaComp = 5)
pca_rfit <- predict(pca_rall, r_all)
pca_rfit$snp_set = "Random"

pca_vall <- preProcess(v_all[,-c(1,2)], method = "pca",  pcaComp = 5)
pca_vfit <- predict(pca_vall, v_all)
pca_vfit$snp_set <- "Volkmann"


pca_complete <- bind_rows(list(pca_fit_all, pca_ifit, pca_rfit, pca_vfit))


pca_complete$Province <- ifelse(pca_complete$Provincial_layer == '26', 
                                "Milne Bay", 
                                ifelse(pca_complete$Provincial_layer =='7',"Madang", "East Sepik"))

pca_complete <- bind_rows(pca_wgs_pred, pca_complete)

p1 <- ggplot(pca_complete, aes(x = PC1, y = PC2, colour = Province)) + 
    facet_wrap(~snp_set, ncol = 5, scales = "free") + geom_point() +
    theme(legend.position = "bottom")

ggsave("pca_snp_sets_PC1vPC2_WGS_only.png", p1)

p2 <- ggplot(pca_complete, aes(x = PC2, y = PC3, colour = Province)) + 
    facet_wrap(~snp_set, ncol = 5, scales = "free") + geom_point() +
    theme(legend.position = "bottom")

ggsave("pca_snp_sets_PC2vPC3_WGS_only.png", p2)

```


1.  iSNPs (p = `r sum(iSNPs)`) -  informative SNPs which were chose as 
having good population differentiation properties (high Fst)
2.  rSNPs (p = `r sum(rSNPs)`)- 'random' SNPs chosen as having low Fst values
3. Volkmann (p = `r sum(vSNPs)`) - SNPs from the _Plasmodium falciparum_ global bar code, 
can overlap with the random SNPs.

To assess the predictive power of these three categories we construct the 
three SVMs with Gaussian kernels on these subsets and see how they compare to
our baseline model that includes all SNPs.

```{r subset_svm}

svm_isnp <- train(Region ~ ., data = i_df, method = "svmRadial", 
                  trControl = fitting_controls, metric = "Kappa")
svm_rsnp <- train(Region ~ ., data = r_df, method = "svmRadial", 
                  trControl = fitting_controls, metric = "Kappa")

svm_vsnp <- train(Region ~ ., data = v_df, method = "svmRadial", 
                  trControl = fitting_controls, metric = "Kappa")

# construct test accuracy
cm_isnps <- confusionMatrix(predict(svm_isnp, itest_df), itest_df$Region)
cm_rsnps <- confusionMatrix(predict(svm_rsnp, rtest_df), rtest_df$Region)
cm_vsnps <- confusionMatrix(predict(svm_vsnp, vtest_df), vtest_df$Region)

byClass_isnps <- data.frame(cm_isnps$byClass) %>% add_rownames("class") %>% 
    mutate(model = "svmGaussian", snps = "iSNPs", n.snps = sum(iSNPs))
byClass_rsnps <- data.frame(cm_rsnps$byClass) %>% add_rownames("class") %>% 
    mutate(model = "svmGaussian", snps =  "rSNPs", n.snps = sum(rSNPs))
byClass_vsnps <- data.frame(cm_vsnps$byClass) %>% add_rownames("class") %>% 
    mutate(model = "svmGaussian", snps = "vSNPs", n.snps = sum(vSNPs))
    
accuracy_df <- data.frame(model = c("svmLinear", "svmGaussian", "iSNP", "rSNP", "vSNP"),
                          n.snps = c(154, 154, sum(iSNPs), sum(rSNPs), sum(vSNPs)),
                          train_accuracy = c(max(svm_fit2$results$Accuracy),
                                             max(svm_fit3$results$Accuracy),
                                             max(svm_isnp$results$Accuracy),
                                             max(svm_rsnp$results$Accuracy),
                                             max(svm_vsnp$results$Accuracy)),
                          test_accuracy = c(cm2$overall[1],
                                            cm3$overall[1],
                                            cm_isnps$overall[1],
                                            cm_rsnps$overall[1],
                                            cm_vsnps$overall[1]),
                          test_accuracy_lower = c(cm2$overall[3],
                                                  cm3$overall[3],
                                                  cm_isnps$overall[3],
                                                  cm_rsnps$overall[3],
                                                  cm_vsnps$overall[3]),
                          test_accuracy_upper = c(cm2$overall[4],
                                                  cm3$overall[4],
                                                  cm_isnps$overall[4],
                                                  cm_rsnps$overall[4],
                                                  cm_vsnps$overall[4]))

knitr::kable(accuracy_df, digits = 2, format = "markdown")

```

These models provide good validation for the chosen SNPs to bar code
PNG infections. The model with only iSNPs performs best (in terms of
prediction accuracy on the test set.) compared to the rSNPs model and vSNPs model.
We also see that the Volkmann bar code has relatively similar performance
to the random SNP set,  implying that this bar code may not be
appropriate for genotyping PNG infections. 

### Pros and Cons of SVM approach

1. Flexible (more classes and/or predictors could be incorporated into model)
2. Reasonably accurate ~  `r round(cm3$overall[1], 2)*100`\%  on test set 
using a Gaussian kernel on all SNPs.
3. Doesn't scale well if $n$ is large.
4. Choosing a minimal set of SNPs is difficult. 
5. Interpretation of the final model difficult.

# Approach 2 - linear discriminant analysis

A simpler approach than SVMs with more interpret-able is to use a multiclass
linear discriminant classifier (multiclass-LDA). It is conceptually very similar
to using PCA, however instead of just selecting axes of the data that contain
the maximum variance of the data, we are also interested in the axes that best
separate ('discriminate') the classes (geographic labels). 
The advantage here is that
we can determine the importance of SNPs based on their correctness in predicting
the geographical labels by looking at linear combinations of SNPs that
best separate the classes. 

First, we fit a multiclass LDA model on the entire SNP
set and then select variables using the resulting eigenvectors of the model.
As before we use 5-fold cross-validation and estimate the confusion matrix on the
test set. 

```{r lda_model1}


lda_fit1 <- train(Region ~ ., data = train_df2, method = "lda",
                  trControl = fitting_controls, metric = "Kappa")
cm_lda1 <- confusionMatrix(predict(lda_fit1, test_df2), 
                           test_df2$Region)

lda_finalmodel <- lda_fit1$finalModel
```

We see that the performance is similar using LDA compared to SVM models
presumably since some of the assumptions of the model i.e. equal covariance 
between classes and normality of data are violated. We have an estimated
training set accuracy of `r max(lda_fit1$results$Accuracy) * 100`\% and
test set accuracy `r round(cm_lda1$overall[1], 2)*100` .

The final model contains 
`r length(lda_finalmodel$svd)`discriminant components that capture approximately
`r paste(100*round(lda_finalmodel$svd^2 / sum(lda_finalmodel$svd^2), 3), collapse = ", ")`
of the variance between classes in the original data, respectively.

If we look at the first discriminant values and project them 
onto our original data, we can visually inspect how well they separate 
the classes.

```{r project, fig.width=12, fig.height=8}
lda_project <- predict(lda_finalmodel, train_df2[,-1])

lda_df <- data.frame(Region = train_df2$Region,
                     lda_project$x)
lda_plot <- GGally::ggpairs(lda_df,columns =  2:ncol(lda_df), 
                aes(colour=Region),
                upper = "blank")
lda_plot
```

The linear discriminants appear to separate out the lowlands samples better than
PCA. We can also use LDA to rank our SNPs by separating them out based on their
final scalings. If the discriminants are close to zero then they won't be contributing
much to the final projection, so if we rank their absolute contributions we
could find a new set of possibly informative SNPs. 

```{r svm_lda}
discriminants <- data.frame(snp_id=rownames(lda_finalmodel$scaling),
                            lda_finalmodel$scaling) %>%
                mutate(rank1 = dense_rank(desc(abs(LD1))),
                       rank2 = dense_rank(desc(abs(LD2))),
                       rank3 = dense_rank(desc(abs(LD3))),
                       rank4 = dense_rank(desc(abs(LD4))),
                       rank5 = dense_rank(desc(abs(LD5))),
                       rank6 = dense_rank(desc(abs(LD6))))

top10_snps <- (discriminants  %>% 
                   filter(rank1 < 10 | 
                              rank2 < 10 |
                              rank3 < 10 | 
                              rank4 < 10 | 
                              rank5 < 10 | 
                              rank6 < 10))$snp_id %>% 
    as.character


```
If we take the top 10 ranked in each
discriminant function, we end up with `r length(top10_snps)` SNPs. 

```{r}
lda_snps <- which(cols %in% top10_snps) + 1

lda_train_df <- train_df2[, c(1, lda_snps)]
lda_test_df <- test_df2[, c(1, lda_snps)]

lda_top10 <- train(Region ~ ., data = lda_train_df, method= "lda", 
                     trControl = fitting_controls, metric = "Kappa")

cm_lda_top10 <- confusionMatrix(predict(lda_top10, lda_test_df), 
                                lda_test_df$Region)
```

Rebuilding our LDA classifier with the top ranking SNPs, we obtain a 
training set accuracy of `r max(lda_top10$results$Accuracy) * 100`\% and
test set accuracy `r round(cm_lda_top10$overall[1], 2)*100`\%.

```{r}

top20_snps <- (discriminants  %>% 
                   filter(rank1 < 20 | 
                              rank2 < 20 |
                              rank3 < 20 | 
                              rank4 < 20 | 
                              rank5 < 20 | 
                              rank6 < 20))$snp_id %>% 
    as.character
lda_snps_20 <- which(cols %in% top20_snps) + 1

lda_train_df <- train_df2[, c(1, lda_snps_20)]
lda_test_df <- test_df2[, c(1, lda_snps_20)]

lda_top20 <- train(Region ~ ., data = lda_train_df, method= "lda", 
                     trControl = fitting_controls, metric = "Kappa")

cm_lda_top20 <- confusionMatrix(predict(lda_top20, lda_test_df), 
                                lda_test_df$Region)
```

Similarly, if we use the same approach choosing the top 20 ranked SNPs in 
each class we get `r length(top20_snps)`. Here we get
training set accuracy of `r max(lda_top20$results$Accuracy) * 100`\% and
test set accuracy `r round(cm_lda_top20$overall[1], 2)*100`\%.

We also try one final approach of projecting the linear discriminants of the
top ranked SNP model and rebuilding our SVM classifier.

```{r lda-project}

train_df3 <- data.frame(Region = train_df2$Region, 
                        predict(lda_top10$finalModel, train_df2[, c(lda_snps)])$x,
                        stringsAsFactors = FALSE)
test_df3 <- data.frame(Region = test_df2$Region,
                       predict(lda_top10$finalModel, test_df2[, c(lda_snps)])$x,
                       stringsAsFactors = FALSE)

combined_svm <- train(Region ~ ., data = train_df3, method= "svmRadial", 
                     trControl = fitting_controls, metric = "Kappa")

cm_combined_svm <- confusionMatrix(predict(combined_svm, test_df3), 
                                test_df3$Region)

```

The results for all LDA classifier models are summarised below:

```{r summarise-lda}
byClass_lda <- data.frame(cm_lda1$byClass) %>% add_rownames("class") %>% 
    mutate(model = "lda", snps = "all", n.snps = 154)
byClass_lda10 <- data.frame(cm_lda_top10$byClass) %>% add_rownames("class") %>% 
    mutate(model = "lda", snps =  "lda-10", n.snps = length(top10_snps))
byClass_lda20 <- data.frame(cm_lda_top20$byClass) %>% add_rownames("class") %>% 
    mutate(model = "lda", snps = "lda-20", n.snps = length(top20_snps))

byClass_lda_svm <- data.frame(cm_combined_svm$byClass) %>% add_rownames("class") %>% 
    mutate(model = "lda-svm", snps = "lda-10", n.snps = length(top10_snps))

lda_models <- bind_rows(byClass_lda, byClass_lda10, byClass_lda20, byClass_lda_svm)

accuracy_df2 <- data.frame(model = c("lda", "lda-top10", "lda-top20", "lda-svm"),
                           n.snps = c(154, length(top10_snps), length(top20_snps), length(top10_snps)),
                          train_accuracy = c(max(lda_fit1$results$Accuracy),
                                             max(lda_top10$results$Accuracy),
                                             max(lda_top20$results$Accuracy),
                                             max(combined_svm$resample$Accuracy)),
                          test_accuracy = c(cm_lda1$overall[1],
                                            cm_lda_top10$overall[1],
                                            cm_lda_top10$overall[1],
                                            cm_combined_svm$overall[1]),
                          test_accuracy_lower = c(cm_lda1$overall[3],
                                            cm_lda_top10$overall[3],
                                            cm_lda_top10$overall[3],
                                            cm_combined_svm$overall[3]),
                          test_accuracy_upper = c(cm_lda1$overall[4],
                                            cm_lda_top10$overall[4],
                                            cm_lda_top10$overall[4],
                                            cm_combined_svm$overall[4]))

knitr::kable(accuracy_df2, digits = 2, format = "markdown")

```

### Pros and Cons of LDA approach

1. LDA assumptions violated by our data set.
2. Provides a simpler interpretation than using SVMs and can be combined
with other methods by fitting models on linear discriminants.
3. Scalable to bigger data-sets.
4. Able to use the linear discriminants for further model building. That is
projecting the data into the LD subspace and retraining.

# Conclusions

## Where to go from here?
We plot the estimated accuracy of the test set for each of our classifiers
ordered by the number of SNPs contained within each set. 

If there is a drop off in accuracy as we increase the size of the SNP set, 
we can use that to determine a set of 'optimal' set of SNPs where the accuracy 
plateaus.

```{r combine_accuracy}
accuracy_all <- bind_rows(accuracy_df, accuracy_df2)

ggplot(accuracy_all, aes(x = n.snps, y = test_accuracy, colour = model)) + geom_point() +
    geom_errorbar(aes(ymin = test_accuracy_lower, ymax = test_accuracy_upper))

```

Again we see that we get best performance using all SNPs with a Gaussian SVM,
however the iSNP model has a similar performance to the LDA models with a much 
smaller number of SNPs.

## Main points

* We have shown that reducing the Fluidigm assay to the set 72 informative SNPs
provides more accurate predictions of geography of infection than using
either random SNPs or the Volkmann SNPs. This was done by building a multiclass
SVM on different SNP subsets. 
* However, the distinction between using the random SNPs or informative
SNPs is not as different as we would hope. 
* We have shown that using all SNPs with an SVM classifier with a Gaussian kernel,
provides good discrimination of the collapsed geographic variables.
* If we use an LDA classifier we get similar results to the SVM. Reducing our
SNP set to the top ranking LDs substantially reduces the number of SNPs required,
with little impact on test set accuracy.
* The LDA classifier has the advantage of being quite flexible and can be improved
as we collect more samples for the underrepresented classes.
* More SNPs are required to distinguish infections between North Coast and
Morobe regions at a finer geographical scale.

# Appendix

## Full model outputs
We print the full output for all estimated confusion matrices for each
model below.

```{r print models}
print("SVM models")
svm_fit1
cm1
svm_fit2
cm2
svm_fit3
cm3
svm_isnp
cm_isnps
svm_rsnp
cm_rsnps
svm_vsnp
cm_vsnps
print("LDA models")
lda_fit1
cm_lda1
lda_top10
cm_lda_top10
lda_top20
cm_lda_top20
combined_svm
combined_cm


```

## Session Information

```{r}
sessionInfo()
```
